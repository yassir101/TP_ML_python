{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports des librairies"
      ],
      "metadata": {
        "id": "ZUSWGAO3Jq1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "bcOKDw1pLaCk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Génération et préparation des données"
      ],
      "metadata": {
        "id": "Tcglwv-7LjcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Génération des données\n",
        "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Conversion en tenseurs PyTorch\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "fKhC2JJ2LsF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversion des données en tenseurs (`torch.tensor`)\n",
        "\n",
        "PyTorch attend des `torch.Tensor`, pas des tableaux NumPy. On convertit donc :\n",
        "- les données d'entrée en `float32`\n",
        "- les labels en `long` (entiers), requis pour `CrossEntropyLoss`\n",
        "\n",
        "```python\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "loh8YuqrLvi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "## Exercice 1 — Définir un MLP\n",
        "\n",
        "Complétez la classe ci-dessous pour créer un MLP avec :\n",
        "- une couche cachée de **32 neurones**\n",
        "- une activation **ReLU**\n",
        "- une sortie à **2 neurones** (car 2 classes)\n"
      ],
      "metadata": {
        "id": "O2XFTfH5L2Eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(2, ___)        # À compléter\n",
        "        self.fc2 = nn.Linear(___, ___)      # À compléter\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.___(self.fc1(x))              # À compléter\n",
        "        return self.fc2(x)\n"
      ],
      "metadata": {
        "id": "xCYFV8ZyL5V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercice 2 — Entraînement du modèle\n",
        "\n",
        "Complétez la boucle pour afficher la **loss** et l'**accuracy** tous les 10 epochs.\n"
      ],
      "metadata": {
        "id": "2voD4jSpMAmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation du modèle\n",
        "model = ___            # Créer une instance du MLP défini précédemment\n",
        "criterion = ___        # Choisir une fonction de perte adaptée à un problème de classification\n",
        "optimizer = ___        # Choisir un optimiseur avec un taux d'apprentissage de 0.01\n",
        "\n",
        "# Paramètres de la boucle d'entraînement\n",
        "n_epochs = ___         # Nombre d’époques (ex : 100)\n",
        "batch_size = ___       # Taille des batchs (ex : 64)\n",
        "train_losses = []\n",
        "\n",
        "# Boucle d'entraînement\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # tirage d'un ordre aléatoire de présentation des exemple dans chaque epoch\n",
        "    permutation = torch.randperm(X_train_tensor.size()[0])\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    for i in range(0, X_train_tensor.size()[0], batch_size):\n",
        "        indices = permutation[i:i + batch_size]\n",
        "\n",
        "        # Extraire les batchs de données et de labels\n",
        "        batch_x = ___\n",
        "        batch_y = ___\n",
        "\n",
        "        # Calcul de la sortie du réseau\n",
        "        outputs = ___\n",
        "\n",
        "        # Calcul de la perte\n",
        "        loss = ___\n",
        "\n",
        "        # Mise à jour des poids\n",
        "        optimizer.___()       # Reset du gradient\n",
        "        ___.backward()        # Calcul du gradient\n",
        "        ___.step()            # Mise à jour des poids\n",
        "\n",
        "        # Suivi de la perte\n",
        "        epoch_loss += ___\n",
        "\n",
        "        # Prédictions et comptage des bonnes réponses\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += ___\n",
        "\n",
        "    # Calcul de l'accuracy globale pour l'époque\n",
        "    accuracy = ___\n",
        "\n",
        "    train_losses.append(epoch_loss)\n",
        "\n",
        "    # Affichage régulier des résultats\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss = {___}, Accuracy = {___}\")\n"
      ],
      "metadata": {
        "id": "px5xHWYMMGio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pourquoi utiliser `torch.no_grad()` ?\n",
        "\n",
        "Lors de l'évaluation (test), on n’a pas besoin de gradients. `torch.no_grad()` permet de :\n",
        "- désactiver le calcul automatique des gradients,\n",
        "- accélérer le calcul,\n",
        "- économiser de la mémoire.\n",
        "\n",
        "```python\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n"
      ],
      "metadata": {
        "id": "Ir4vY6VTMLaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **Évaluation du modèle**\n"
      ],
      "metadata": {
        "id": "UHdlPGAuMO1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluation du modèle (à compléter si besoin)\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    acc = accuracy_score(y_test, predicted.numpy())\n",
        "    print(f\"Test accuracy : {acc:.2f}\")"
      ],
      "metadata": {
        "id": "J73IlbjOMM19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Affichage de la frontière de décisions"
      ],
      "metadata": {
        "id": "B03Hw8iEM-X6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_decision_boundary(model, X, y):\n",
        "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    grid_t = torch.tensor(grid, dtype=torch.float32)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        preds = model(grid_t)\n",
        "        _, pred_labels = torch.max(preds, 1)\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.contourf(xx, yy, pred_labels.numpy().reshape(xx.shape), cmap='coolwarm', alpha=0.6)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
        "    plt.title(\"Frontière de décision du modèle\")\n",
        "    plt.show()\n",
        "\n",
        "# Appel de la fonction avec les données de test\n",
        "plot_decision_boundary(model, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "DF-6OiTPNGtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Affichage des points mal classés"
      ],
      "metadata": {
        "id": "LEKbrYDyNJbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    incorrect = predicted != y_test_tensor\n",
        "    incorrect_indices = np.where(incorrect.numpy())[0]\n",
        "\n",
        "print(f\"Nombre de points mal classés : {len(incorrect_indices)}\")\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='coolwarm', edgecolors='k', alpha=0.3, label='Vrai label')\n",
        "plt.scatter(X_test[incorrect_indices, 0], X_test[incorrect_indices, 1], c='black', label='Erreur')\n",
        "plt.legend()\n",
        "plt.title(\"Points mal classés par le modèle\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SEiEvj-iNQg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercice 3 — Comparaison de différentes architectures\n",
        "\n",
        "On souhaite comparer :\n",
        "- un modèle **sous-ajusté** (trop simple)\n",
        "- un modèle **sur-ajusté** (trop complexe)\n",
        "- un modèle **régularisé** (avec Dropout)\n",
        "\n",
        "Complétez les architectures suivantes puis entraînez-les comme dans l'Exercice 2.\n",
        "Comparez leur précision, courbe de perte et frontière de décision.\n",
        "\n",
        "Vous pourrez ensuite tester d'autres architectures encore.\n"
      ],
      "metadata": {
        "id": "3uGyNmORRVyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modèle sous-ajusté : une seule couche linéaire\n",
        "class UnderfitMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(2, ___)  # À compléter\n",
        "\n",
        "    def forward(self, x):\n",
        "        return ___                   # À compléter\n"
      ],
      "metadata": {
        "id": "Ypn5EZG4Sg-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modèle sur-ajusté : deux couches larges\n",
        "class OverfitMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(2, ___)    # À compléter\n",
        "        self.fc2 = nn.Linear(___, ___) # À compléter\n",
        "        self.fc3 = nn.Linear(___, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.___(self.fc1(x))         # À compléter\n",
        "        x = F.___(self.fc2(x))         # À compléter\n",
        "        return self.fc3(x)\n"
      ],
      "metadata": {
        "id": "qXOzE1pBSpRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modèle régularisé : avec Dropout\n",
        "class DropoutMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(2, ___)        # À compléter\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(___, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.___(self.fc1(x))              # À compléter\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)\n"
      ],
      "metadata": {
        "id": "TswqbCd-SrW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercice 4 — Calcul de la loss de validation\n",
        "\n",
        "Complétez le code ci-dessous pour :\n",
        "- ajouter une séparation du jeu d’entraînement en **train** et **validation**\n",
        "- calculer la **loss de validation** à chaque époque\n",
        "- afficher la loss de validation tous les 10 epochs\n",
        "\n",
        "Cela permet de suivre le comportement du modèle sur des données non vues pendant l'entraînement.\n"
      ],
      "metadata": {
        "id": "VJ9eG9cpTCGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nouvelle séparation : X_train → X_subtrain + X_val\n",
        "X_subtrain, X_val, y_subtrain, y_val = train_test_split(\n",
        "    X_train_tensor, y_train_tensor, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Entraînement sur X_subtrain\n",
        "# Ajouter calcul de la loss sur X_val à chaque epoch\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # boucle d'entraînement classique (sur X_subtrain)\n",
        "    # ...\n",
        "\n",
        "    # Partie à compléter : calcul de la loss de validation\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(___)             # À compléter\n",
        "        val_loss = criterion(val_outputs, ___)  # À compléter\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Validation loss = {___}\")  # À compléter\n"
      ],
      "metadata": {
        "id": "pB1wW0JKTBxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afficher la loss en apprentissage et en validation en fonction des epochs"
      ],
      "metadata": {
        "id": "y_ttccPfVr5U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X54G1Y3yVyhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pour aller plus loin\n",
        "\n",
        "Dans un nouveau notebook, définir un nouveau MLP pour travailler sur les données ``digits`̀  de scikit-learn"
      ],
      "metadata": {
        "id": "XN0D3O-FWKfW"
      }
    }
  ]
}